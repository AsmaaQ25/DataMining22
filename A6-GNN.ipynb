{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca78291",
   "metadata": {
    "id": "POwDaceEsKCg"
   },
   "source": [
    "# CISC 873 Data Mining Competition #6\n",
    "name: Asmaa Qindeel\n",
    "\n",
    "\n",
    "Competition #6: https://www.kaggle.com/competitions/cisc873-dm-f22-a6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2645bc1",
   "metadata": {
    "id": "e_poxWuQsUYP"
   },
   "source": [
    "# TOPICS:\n",
    "* [Questions & Answers](#Questions)\n",
    "* [Problem and Protocol](#intro)\n",
    "\n",
    "**Code Part**\n",
    "* [Preparing workspace: gathering info about the data](#pre)\n",
    "\n",
    "* [Trail 1:](#t1)\n",
    "* [Trail 2:](#t2)\n",
    "* [Trail 3:](#t3)\n",
    "* [Trail 4:](#t4)\n",
    "* [Trail 5:](#t5)\n",
    "* [Trial 6:](#t6)\n",
    "* [Trial 7:](#t7)\n",
    "* [Trial 8:](#t8)\n",
    "* [Trial 9:](#t9)\n",
    "* [Trial 10:](#t10)\n",
    "\n",
    "\n",
    "\n",
    "[**Conclusion**](#conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7628b",
   "metadata": {},
   "source": [
    "You have to tune at least 10 times. All the tried solutions should be different (e.g. different feature sets/different preprocessing). The tried solutions should cover at least:\n",
    "\n",
    "GCN aggregation mechanisms: Tune at least three aggregation mechanisms (aka message_passing mechanisms) used in the graph convolution layer. Explain your understanding (based on the documentation/paper) of the mechanisms you chose.\n",
    "\n",
    "\n",
    "Up-sampling: adjust the training data preparation/generation part to up-sample the positive class samples (very unbalanced dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d721248",
   "metadata": {
    "id": "fex7AHnxsXuN"
   },
   "source": [
    "<a class=\"anchor\" id=\"Questions\"></a>\n",
    "\n",
    "# Questions:  \n",
    "\n",
    "1. Based on the provided template, describe the format of the input file (sdf file).\n",
    "    - the data is in a Spatial Data File(sdf), means every dataitem is written in a specific place with specific order. Knowing that order is the key to read the file. For instance, in our file the samples are separated by `$$$$`, first line is the id, last line is the label, one column for the nodes, two columns for the edges, one column for edge weights. And a descriptive line, the second line which has the data: (number_of_nodes number_of_edges) in the first two items.\n",
    "2. What are the input tensors to the neural network model (their meaning, not just symbol)? What is each of their dims and their meaning (e.g. batch_size)?\n",
    "    - three input tensors\n",
    "        - data(m,): the nodes, which are letters that we tokenize and embed later. dim = m, where m = batch_size * number of nodes in the biggest sample. If batch_size =1, then m = nodes of sample.\n",
    "        - edge(n,2): the edges of all the samples in the batch. dim = n * 2, where n is the total number of edges in all the batch samples, and 2 because each sample needs 2 items to descripe one edge.\n",
    "        - node2graph(m): is an indicator of the sample order/index in the batch. m is same m as in data.\n",
    "        \n",
    "3. For each dim of gnn_out, what does it symbolize? For each dim of avg, what does it symbolize?\n",
    "    - gnn_out(m,32): m as above, with 32 hidden_dim which is the hidden nodes. avg(batch_size,32) takes the mean for each sample, the mean per segment. it reduces the m* 32 dim into batch_size * 32.\n",
    "\n",
    "4. What is the difference between segment_mean and tf.reduce_mean? For each dim of pred, what does it symbolize?\n",
    "    - segment mean may have different size for each segment. reduce_mean splits the input in same sized sets. pred(batch_size, 1), first dim is the batch size or the samples number. second dim is the label dim, since this is binary classification, the output layer has a single node.\n",
    "5. What is the motivation/theory/idea to use multiple gcn layers comparing to just one? How many layers were used in the template?\n",
    "    - probably deeper graphs, in here we use 1 GCN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fab6ca",
   "metadata": {
    "id": "7mo66ko2sgbR"
   },
   "source": [
    "<a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "# Problem Formulation:\n",
    "https://www.kaggle.com/c/cisc-873-dm-f22-a3\n",
    "\n",
    "THis is a classifidation problem, where each input is a graph of a chemical compound. the compound can be classifien as positive or negative for medical purposes.\n",
    "\n",
    "The input will be in a sdf file format, it needs to be read in order. \n",
    "\n",
    "Metric used in this problem is roc_auc.\n",
    "\n",
    "## The Impact:\n",
    "Training a model to classify chemical compounds can be used as an attachment to scientific equipment, i.e. make an automatic microscope that can read the sample and identify it with no human error, wouldn't that be nice!\n",
    "\n",
    "There is a project in AI that is trending now, it is about predicting protien folding, I don't realy understand its impact, but they say it will make a great change in the medical field. such a project also works on graph data input. So including graphs in machine learning is cruicial, not all data is rows and columns. \n",
    "\n",
    "\n",
    "# Experimental Protocol:\n",
    "\n",
    "Read the sdf file, save the data.\n",
    "\n",
    "For the nodes in graphs We will tokenize and embed them(handle them like text NLP), then the edges will be handled as numeric features.\n",
    "\n",
    "liberary tf2-gnn is used to include the layers for graph learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64078d8b",
   "metadata": {
    "id": "f808a0ed"
   },
   "source": [
    "# References\n",
    "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. Oâ€™Reilly Media Inc.\n",
    "\n",
    "[Competition Template](#https://www.kaggle.com/competitions/cisc873-dm-f22-a6/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7eecf0",
   "metadata": {
    "executionInfo": {
     "elapsed": 2869,
     "status": "ok",
     "timestamp": 1647248501884,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "4ec5d4e8"
   },
   "outputs": [],
   "source": [
    "# import libs\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import segment_mean\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#the lib for graph convolution\n",
    "from tf2_gnn.layers.gnn import GNN, GNNInput\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5fef2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2050,
     "status": "ok",
     "timestamp": 1647248539031,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "644a065b",
    "outputId": "6b505b5a-4ea1-4e7a-b4ea-f83033742757"
   },
   "outputs": [],
   "source": [
    "#load the data\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def read_sdf(file):\n",
    "    #open the file\n",
    "    with open(file, 'r') as rf:\n",
    "        content = rf.read()\n",
    "    samples = content.split('$$$$') # the sign between each two samples\n",
    "    \n",
    "    def parse_sample(s):\n",
    "        #read lines in current sample\n",
    "        lines = s.splitlines()\n",
    "        links = []\n",
    "        nodes = []\n",
    "        label = 0\n",
    "        for l in lines:\n",
    "            #read the label line\n",
    "            if l.strip() == '1.0':\n",
    "                label = 1\n",
    "            if l.strip() == '-1.0':\n",
    "                label = 0\n",
    "            # read the data lines, they start with four spaces\n",
    "            if l.startswith('    '):\n",
    "                feature = l.split()\n",
    "                node = feature[3] \n",
    "                nodes.append(node)\n",
    "            # read the edges lines, they start with one space\n",
    "            elif l.startswith(' '):\n",
    "                lnk = l.split()\n",
    "                # edge: (from, to,) (1-based index)\n",
    "                if int(lnk[0]) - 1 < len(nodes):\n",
    "                    links.append((\n",
    "                        int(lnk[0])-1, \n",
    "                        int(lnk[1])-1, # zero-based index\n",
    "                        # int(lnk[2]) ignore edge weight\n",
    "                    ))\n",
    "        return nodes, np.array(links), label\n",
    "    \n",
    "    return [parse_sample(s) for s in tqdm(samples) if len(s[0]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3add5d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c57ccd8d54a4708a2633845b5ece5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the data to test and validation\n",
    "training_set = read_sdf('train.sdf')\n",
    "training_set, validation_set = train_test_split(training_set, test_size=0.15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bca9992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcd0e7e5f4c4c7fb765acaf33da2177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the test file as well\n",
    "testing_set  = read_sdf('test_x.sdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d401428",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"pre\"></a>\n",
    "\n",
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68df7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# tokenize the nodes \n",
    "\n",
    "max_vocab = 500\n",
    "max_len = 100\n",
    "\n",
    "# build vocabulary from training set\n",
    "all_nodes = [s[0] for s in training_set]\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba9c97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "def prepare_single_batch(samples):\n",
    "    '''function to prepare single batch, embed all nodes together, \n",
    "    get all edges together, and add node2graph index for each node\n",
    "    '''\n",
    "    sample_nodes = [s[0] for s in samples]\n",
    "    sample_nodes = tokenizer.texts_to_sequences(sample_nodes)\n",
    "    sample_nodes = pad_sequences(sample_nodes, padding='post')\n",
    "    max_nodes_len = np.shape(sample_nodes)[1] #biggest node number in the samples\n",
    "    edges = [s[1]+i*max_nodes_len for i,s in enumerate(samples)]\n",
    "    edges = [e for e in edges if len(e) > 0]\n",
    "    node_to_graph = [[i]*max_nodes_len for i in range(len(samples))]\n",
    "    \n",
    "    all_nodes = np.reshape(sample_nodes, -1)\n",
    "    all_edges = np.concatenate(edges)\n",
    "\n",
    "    node_to_graph = np.reshape(node_to_graph, -1)\n",
    "    return {\n",
    "        'data': all_nodes,\n",
    "        'edges': all_edges,\n",
    "        'node2grah': node_to_graph,\n",
    "    }, np.array([s[2] for s in samples])\n",
    "\n",
    "\n",
    "\n",
    "def gen_batch(dataset, batch_size=16, repeat=False, shuffle=True):\n",
    "    while True:\n",
    "        dataset = list(dataset)\n",
    "        if shuffle:\n",
    "            random.shuffle(dataset)\n",
    "        l = len(dataset)\n",
    "        for ndx in range(0, l, batch_size):\n",
    "            batch_samples = dataset[ndx:min(ndx + batch_size, l)]\n",
    "            yield prepare_single_batch(batch_samples)\n",
    "        if not repeat:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f518d",
   "metadata": {
    "id": "651431da"
   },
   "source": [
    "<a class=\"anchor\" id=\"t1\"></a>\n",
    "\n",
    "# Trial 1:\n",
    "use model with one gcn layer and one dense layer, with defalut parameters.\n",
    "only change the hidden_dim of the gcn layer to 32\n",
    "\n",
    "- hypers: defalut optimizer(rmsprop)\n",
    "- batch_size = 16\n",
    "- epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8753fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  ()                  0           ['input_3[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 20)           10000       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  ()                  0           ['tf.math.reduce_max[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " gnn (GNN)                      (None, 32)           22464       ['embedding[0][0]',              \n",
      "                                                                  'input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]',                \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.segment_mean (TFOpLamb  (None, 32)          0           ['gnn[0][0]',                    \n",
      " da)                                                              'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            33          ['tf.math.segment_mean[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,497\n",
      "Trainable params: 32,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "model = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c25d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile loss function\n",
    "model.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d978c652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/Identity:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_1_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_1_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/Identity:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgcn/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330/1330 [==============================] - 28s 18ms/step - loss: 0.2276 - auc: 0.4728 - val_loss: 0.2146 - val_auc: 0.5856\n",
      "Epoch 2/5\n",
      "1330/1330 [==============================] - 23s 18ms/step - loss: 0.2006 - auc: 0.5667 - val_loss: 0.2111 - val_auc: 0.6138\n",
      "Epoch 3/5\n",
      "1330/1330 [==============================] - 22s 17ms/step - loss: 0.1949 - auc: 0.6133 - val_loss: 0.2064 - val_auc: 0.6683\n",
      "Epoch 4/5\n",
      "1330/1330 [==============================] - 24s 18ms/step - loss: 0.1882 - auc: 0.6458 - val_loss: 0.2011 - val_auc: 0.6729\n",
      "Epoch 5/5\n",
      "1330/1330 [==============================] - 23s 17ms/step - loss: 0.1826 - auc: 0.6849 - val_loss: 0.1974 - val_auc: 0.7017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170a32da070>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train teh model\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=5,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6affd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09073231, 0.05983728, 0.05978483, ..., 0.019364  , 0.09987938,\n",
       "       0.02540815], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predic and save to file\n",
    "y_pred = model.predict(\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ")\n",
    "\n",
    "y_pred = np.reshape(y_pred, -1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99659e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('T1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517aa5e0",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"t2\"></a>\n",
    "# Trial 2: \n",
    "\n",
    "\n",
    "T1 got 0.7 val_accuracy, which is a good start, next i'll try changing one parameter of the model. it is the message passing style, meaning how the model will handle each node in graph, the defalut is `rgcn`, lets change it to `ggnn`.\n",
    "\n",
    "**setting:**\n",
    "\n",
    "- one gcn layer with hidden_dim=32\n",
    "- one dense layer\n",
    "- hypers: defalut optimizer(rmsprop)\n",
    "- batch_size = 16\n",
    "- epoch = 10 + add early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8384d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rgcn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default message_calculation_class \n",
    "params['message_calculation_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b5ac8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_2 (TFOpLamb  ()                  0           ['input_9[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 20)           10000       ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  ()                  0           ['tf.math.reduce_max_2[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_2 (GNN)                    (None, 32)           47808       ['embedding_2[0][0]',            \n",
      "                                                                  'input_8[0][0]',                \n",
      "                                                                  'input_9[0][0]',                \n",
      "                                                                  'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_1 (TFOpLa  (None, 32)          0           ['gnn_2[0][0]',                  \n",
      " mbda)                                                            'input_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            33          ['tf.math.segment_mean_1[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57,841\n",
      "Trainable params: 57,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "params['message_calculation_class'] = 'ggnn'\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    " \n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    " \n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "model2 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb4175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model2.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe56ad30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn_3/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn_3/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/ggnn_3/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn_2/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/ggnn_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn_1/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn_1/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/ggnn_1/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/ggnn/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/ggnn/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/ggnn/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330/1330 [==============================] - 42s 27ms/step - loss: 0.1902 - auc: 0.6182 - val_loss: 0.2053 - val_auc: 0.6378\n",
      "Epoch 2/10\n",
      "1330/1330 [==============================] - 35s 26ms/step - loss: 0.1832 - auc: 0.6569 - val_loss: 0.2259 - val_auc: 0.6379\n",
      "Epoch 3/10\n",
      "1330/1330 [==============================] - 36s 27ms/step - loss: 0.1828 - auc: 0.6565 - val_loss: 0.1990 - val_auc: 0.6763\n",
      "Epoch 4/10\n",
      "1330/1330 [==============================] - 38s 29ms/step - loss: 0.1777 - auc: 0.6947 - val_loss: 0.1886 - val_auc: 0.7265\n",
      "Epoch 5/10\n",
      "1330/1330 [==============================] - 36s 27ms/step - loss: 0.1756 - auc: 0.6979 - val_loss: 0.1966 - val_auc: 0.7129\n",
      "Epoch 6/10\n",
      "1330/1330 [==============================] - 36s 27ms/step - loss: 0.1723 - auc: 0.7193 - val_loss: 0.1881 - val_auc: 0.7309\n",
      "Epoch 7/10\n",
      "1330/1330 [==============================] - 36s 27ms/step - loss: 0.1707 - auc: 0.7277 - val_loss: 0.1943 - val_auc: 0.7417\n",
      "Epoch 8/10\n",
      "1330/1330 [==============================] - 36s 27ms/step - loss: 0.1693 - auc: 0.7410 - val_loss: 0.1817 - val_auc: 0.7701\n",
      "Epoch 9/10\n",
      "1330/1330 [==============================] - 35s 26ms/step - loss: 0.1696 - auc: 0.7249 - val_loss: 0.1884 - val_auc: 0.7348\n",
      "Epoch 10/10\n",
      "1330/1330 [==============================] - 35s 26ms/step - loss: 0.1667 - auc: 0.7347 - val_loss: 0.1846 - val_auc: 0.7486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170aca402b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model2.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=10,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode = 'min',\n",
    "    patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd30df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and save to file\n",
    "y_pred = model2.predict(\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ")\n",
    "y_pred = np.reshape(y_pred, -1)\n",
    "\n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('T2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08f9d3",
   "metadata": {
    "id": "OBFqwoQC5qEz"
   },
   "source": [
    "<a class=\"anchor\" id=\"t3\"></a>\n",
    "\n",
    "# Trial 3:\n",
    "\n",
    "from trial 1 and 2, `ggnn` is better, also the 5 epochs were not enough. but we are still exploring which method is better, here lets try the `rgin`, which requires a num_aggr_MLP_hidden_layers \n",
    "\n",
    "**new setting:**\n",
    "- one gcn layer with hidden_dim=32\n",
    "- one dense layer\n",
    "- hypers: defalut optimizer(rmsprop)\n",
    "- batch_size = 16\n",
    "- epoch = 10 + add early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1526341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_3 (TFOpLamb  ()                  0           ['input_12[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 20)           10000       ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  ()                  0           ['tf.math.reduce_max_3[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_3 (GNN)                    (None, 32)           92096       ['embedding_3[0][0]',            \n",
      "                                                                  'input_11[0][0]',               \n",
      "                                                                  'input_12[0][0]',               \n",
      "                                                                  'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_2 (TFOpLa  (None, 32)          0           ['gnn_3[0][0]',                  \n",
      " mbda)                                                            'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['tf.math.segment_mean_2[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 102,129\n",
      "Trainable params: 102,129\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "\n",
    "params['message_calculation_class'] = 'rgin'\n",
    "\n",
    "# change the num_aggr_MLP_hidden_layers\n",
    "params['num_aggr_MLP_hidden_layers'] = 16\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "\n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "\n",
    "model3 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a26185cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63921f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgin_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgin_2/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgin_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgin/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgin/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgin/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330/1330 [==============================] - 73s 47ms/step - loss: 0.2299 - auc: 0.5480 - val_loss: 0.2041 - val_auc: 0.6796\n",
      "Epoch 2/10\n",
      "1330/1330 [==============================] - 84s 64ms/step - loss: 0.2144 - auc: 0.5689 - val_loss: 0.1917 - val_auc: 0.6411\n",
      "Epoch 3/10\n",
      "1330/1330 [==============================] - 76s 57ms/step - loss: 0.2046 - auc: 0.5960 - val_loss: 0.2114 - val_auc: 0.6437\n",
      "Epoch 4/10\n",
      "1330/1330 [==============================] - 63s 47ms/step - loss: 0.2156 - auc: 0.5186 - val_loss: 0.2247 - val_auc: 0.3724\n",
      "Epoch 5/10\n",
      "1330/1330 [==============================] - 74s 55ms/step - loss: 0.2079 - auc: 0.5179 - val_loss: 0.2092 - val_auc: 0.6564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d6ff656970>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model3.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=10,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode = 'min',\n",
    "    patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d264857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and save to file \n",
    "y_pred = model3.predict(\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ")\n",
    "y_pred = np.reshape(y_pred, -1)\n",
    "\n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('T3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24e8fb",
   "metadata": {
    "id": "OBFqwoQC5qEz"
   },
   "source": [
    "<a class=\"anchor\" id=\"t4\"></a>\n",
    "\n",
    "# Trial 4: Sampling:\n",
    "\n",
    "As mentioned in the https://www.kaggle.com/competitions/cisc873-dm-f22-a6/overview , the data is unbalanced, so here i try to balance it with simply repeating the minor class nodes.\n",
    "\n",
    "**new setting:**\n",
    "- sampling strategy is 1:2 , so that i don't overflood the data with repeated dummy samples.\n",
    "- one gcn layer with hidden_dim=32\n",
    "- defalus message_calculation_class = 'rgcn'\n",
    "- one dense layer\n",
    "- hypers: defalut optimizer(rmsprop)\n",
    "- batch_size = 16\n",
    "- epoch = 10 + add early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44ce46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn.over_sampling as over_sampling \n",
    "import imblearn.under_sampling as under_sampling\n",
    "import imblearn.combine as combine\n",
    "import imblearn.pipeline as imb_pipe\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3235a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 20254, 1: 1016})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check balance\n",
    "# turn data to data frame to easily duplicate the minority class later\n",
    "df = pd.DataFrame(training_set)\n",
    "X = df.iloc[:, : -1].values\n",
    "y = df.iloc[:, -1].values\n",
    " \n",
    "# use counter to count the classes # i could use pandas.\n",
    "c = Counter(y)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c08e31",
   "metadata": {},
   "source": [
    "data ratio of 5:100 , this is enough to render the model biased for the major class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e9fb0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20254\n",
       "1    10160\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 9*duplicates , plus original will be 10\n",
    "class_oversample = df.loc[df[df[2]==1].index.repeat(9)]\n",
    "\n",
    "df_over = pd.concat([df, class_oversample])\n",
    "\n",
    "#check classes now\n",
    "df_over[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b21ea390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back in the list\n",
    "training_set_over = df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ee5d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_4 (TFOpLamb  ()                  0           ['input_15[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 20)           10000       ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  ()                  0           ['tf.math.reduce_max_4[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_4 (GNN)                    (None, 32)           22464       ['embedding_4[0][0]',            \n",
      "                                                                  'input_14[0][0]',               \n",
      "                                                                  'input_15[0][0]',               \n",
      "                                                                  'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_3 (TFOpLa  (None, 32)          0           ['gnn_4[0][0]',                  \n",
      " mbda)                                                            'input_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            33          ['tf.math.segment_mean_3[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,497\n",
      "Trainable params: 32,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "model4 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59c15a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(\n",
    "    #optimizer is rmsprop by defalut\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df67a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1905/1905 [==============================] - 38s 18ms/step - loss: 0.5835 - auc: 0.6929 - val_loss: 0.4033 - val_auc: 0.7665\n",
      "Epoch 2/10\n",
      "1905/1905 [==============================] - 32s 17ms/step - loss: 0.5416 - auc: 0.7547 - val_loss: 0.3566 - val_auc: 0.7563\n",
      "Epoch 3/10\n",
      "1905/1905 [==============================] - 32s 17ms/step - loss: 0.5249 - auc: 0.7771 - val_loss: 0.3587 - val_auc: 0.7668\n",
      "Epoch 4/10\n",
      "1905/1905 [==============================] - 34s 18ms/step - loss: 0.5142 - auc: 0.7898 - val_loss: 0.3541 - val_auc: 0.7712\n",
      "Epoch 5/10\n",
      "1905/1905 [==============================] - 38s 20ms/step - loss: 0.5079 - auc: 0.7973 - val_loss: 0.3349 - val_auc: 0.7733\n",
      "Epoch 6/10\n",
      "1905/1905 [==============================] - 35s 18ms/step - loss: 0.5000 - auc: 0.8045 - val_loss: 0.3998 - val_auc: 0.7915\n",
      "Epoch 7/10\n",
      "1905/1905 [==============================] - 35s 18ms/step - loss: 0.4951 - auc: 0.8091 - val_loss: 0.3753 - val_auc: 0.8023\n",
      "Epoch 8/10\n",
      "1905/1905 [==============================] - 32s 17ms/step - loss: 0.4831 - auc: 0.8203 - val_loss: 0.3334 - val_auc: 0.8183\n",
      "Epoch 9/10\n",
      "1905/1905 [==============================] - 32s 17ms/step - loss: 0.4782 - auc: 0.8252 - val_loss: 0.3524 - val_auc: 0.8049\n",
      "Epoch 10/10\n",
      "1905/1905 [==============================] - 32s 17ms/step - loss: 0.4722 - auc: 0.8302 - val_loss: 0.4389 - val_auc: 0.7918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279e1245c40>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set_over) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model4.fit(\n",
    "    gen_batch(\n",
    "        training_set_over, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=10,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "     callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode = 'min',\n",
    "    patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "23b361d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model4.predict(\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ")\n",
    "print(y_pred.shape)\n",
    "y_pred = np.reshape(y_pred, -1)\n",
    "print(y_pred.shape)\n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('T4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee45f62",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"t5\"></a>\n",
    "\n",
    "# Trial 5:\n",
    "val_acc = 0.79, Seems like oversamplig to ratio 1:2 made a big change.\n",
    "\n",
    "I'll try another sampling method, the SMOTE. According to this https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "it repeates certain points.\n",
    "\n",
    "\n",
    "**trial failed** i couldn't get the SMOTE model to take the graph data.\n",
    "\n",
    "I also tried to create a generator like the gen_batch for the SMOTE model, but that faile too. Tried to build my own smart resampler, also failed. T5 failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58cf6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pipline = imb_pipe(\n",
    "#     steps=[       \n",
    "#         ('resampler' , over_sampling.BorderlineSMOTE(sampling_strategy=0.5)), #over sampler\n",
    "#         ('my_classifier', model4())]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "733258b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = pd.DataFrame(training_set)\n",
    "# oversampler = over_sampling.SMOTE(sampling_strategy=0.5)\n",
    "# X = oversampler.fit_resample(training_set)\n",
    "# training_set\n",
    "# Counter(training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d0a49e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resampler(samples):\n",
    "#     sample_nodes = [s[0] for s in samples]\n",
    "#     sample_nodes = tokenizer.texts_to_sequences(sample_nodes)\n",
    "#     sample_nodes = pad_sequences(sample_nodes, padding='post')\n",
    "#     max_nodes_len = np.shape(sample_nodes)[1]\n",
    "#     edges = [s[1]+i*max_nodes_len for i,s in enumerate(samples)]\n",
    "#     edges = [e for e in edges if len(e) > 0]\n",
    "# #     node_to_graph = [[i]*max_nodes_len for i in range(len(samples))]\n",
    "    \n",
    "#     all_nodes = np.reshape(sample_nodes, -1)\n",
    "#     all_edges = np.concatenate(edges)\n",
    "\n",
    "# #     node_to_graph = np.reshape(node_to_graph, -1)\n",
    "#     return {\n",
    "#         'data': all_nodes,\n",
    "#         'edges': all_edges,\n",
    "# #         'node2grah': node_to_graph,\n",
    "#     }, np.array([s[2] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6a0f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resampler\n",
    "# def iresampler(dataset, batch_size=1, repeat=False, shuffle=False):\n",
    "#     while True:\n",
    "#         dataset = list(dataset)\n",
    "#         if shuffle:\n",
    "#             random.shuffle(dataset)\n",
    "#         l = len(dataset)\n",
    "#         x_dict = {}\n",
    "#         y_array = np.array(())\n",
    "#         for ndx in range(0, l, batch_size):\n",
    "#             batch_samples = dataset[ndx:min(ndx + batch_size, l)]\n",
    "#             x, y = resampler(batch_samples)\n",
    "#             np.concatenate((x_dict, x))\n",
    "            \n",
    "#         if not repeat:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f311d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = over_sampling.SMOTE(sampling_strategy=0.5)\n",
    "# # X, y = sampler.fit_resample(prepare_single_batch(training_set))\n",
    "# X = resampler(training_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cce8bf",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"t6\"></a>\n",
    "\n",
    "# Trial 6:\n",
    "\n",
    "Same setting as T4, but change the optimizer, the default rmsprop is not converging fast enough\n",
    "\n",
    "**setting:**\n",
    "- sampling strategy is 1:2 , so that i don't overflood the data with repeated dummy samples.\n",
    "- one gcn layer with hidden_dim=32\n",
    "- defalut message_calculation_class = 'rgcn'\n",
    "- one dense layer\n",
    "- hypers: defalut optimizer(rmsprop)\n",
    "- batch_size = 16\n",
    "- epoch = 15 + add early stopping\n",
    "\n",
    "**change**\n",
    "optimizer : adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c573a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_5 (TFOpLamb  ()                  0           ['input_18[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 20)           10000       ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  ()                  0           ['tf.math.reduce_max_5[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " gnn_5 (GNN)                    (None, 32)           22464       ['embedding_5[0][0]',            \n",
      "                                                                  'input_17[0][0]',               \n",
      "                                                                  'input_18[0][0]',               \n",
      "                                                                  'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_4 (TFOpLa  (None, 32)          0           ['gnn_5[0][0]',                  \n",
      " mbda)                                                            'input_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            33          ['tf.math.segment_mean_4[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,497\n",
      "Trainable params: 32,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    " \n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "model6 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec5147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.compile( optimizer='adam',\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bd30ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1330/1330 [==============================] - 26s 19ms/step - loss: 0.1767 - auc: 0.7087 - val_loss: 0.1941 - val_auc: 0.7190\n",
      "Epoch 2/15\n",
      "1330/1330 [==============================] - 25s 18ms/step - loss: 0.1757 - auc: 0.7083 - val_loss: 0.1913 - val_auc: 0.7283\n",
      "Epoch 3/15\n",
      "1330/1330 [==============================] - 23s 18ms/step - loss: 0.1723 - auc: 0.7316 - val_loss: 0.1883 - val_auc: 0.7211\n",
      "Epoch 4/15\n",
      "1330/1330 [==============================] - 25s 19ms/step - loss: 0.1706 - auc: 0.7418 - val_loss: 0.1913 - val_auc: 0.7740\n",
      "Epoch 5/15\n",
      "1330/1330 [==============================] - 23s 18ms/step - loss: 0.1700 - auc: 0.7447 - val_loss: 0.1890 - val_auc: 0.7141\n",
      "Epoch 6/15\n",
      "1330/1330 [==============================] - 25s 19ms/step - loss: 0.1685 - auc: 0.7531 - val_loss: 0.1813 - val_auc: 0.7696\n",
      "Epoch 7/15\n",
      "1330/1330 [==============================] - 23s 17ms/step - loss: 0.1670 - auc: 0.7628 - val_loss: 0.2033 - val_auc: 0.7359\n",
      "Epoch 8/15\n",
      "1330/1330 [==============================] - 24s 18ms/step - loss: 0.1673 - auc: 0.7590 - val_loss: 0.1934 - val_auc: 0.7365\n",
      "Epoch 9/15\n",
      "1330/1330 [==============================] - 23s 17ms/step - loss: 0.1679 - auc: 0.7554 - val_loss: 0.2007 - val_auc: 0.7630\n",
      "Epoch 10/15\n",
      "1330/1330 [==============================] - 24s 18ms/step - loss: 0.1655 - auc: 0.7715 - val_loss: 0.1862 - val_auc: 0.7552\n",
      "Epoch 11/15\n",
      "1330/1330 [==============================] - 27s 20ms/step - loss: 0.1664 - auc: 0.7675 - val_loss: 0.1876 - val_auc: 0.7613\n",
      "Epoch 12/15\n",
      "1330/1330 [==============================] - 24s 18ms/step - loss: 0.1642 - auc: 0.7751 - val_loss: 0.1820 - val_auc: 0.7559\n",
      "Epoch 13/15\n",
      "1330/1330 [==============================] - 24s 18ms/step - loss: 0.1646 - auc: 0.7779 - val_loss: 0.1947 - val_auc: 0.7273\n",
      "Epoch 14/15\n",
      "1330/1330 [==============================] - 23s 17ms/step - loss: 0.1642 - auc: 0.7817 - val_loss: 0.1803 - val_auc: 0.7885\n",
      "Epoch 15/15\n",
      "1330/1330 [==============================] - 23s 17ms/step - loss: 0.1622 - auc: 0.7876 - val_loss: 0.1773 - val_auc: 0.7697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170b70dd8b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set_over) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model6.fit(\n",
    "    gen_batch(\n",
    "        training_set_over, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=15,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    "     callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode = 'min',\n",
    "    patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "adb108c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict and save to file\n",
    "y_pred = model6.predict(\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ")\n",
    "\n",
    "y_pred = np.reshape(y_pred, -1)\n",
    "\n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('T6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0c4f0",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"t7\"></a>\n",
    "\n",
    "# Trial 7:\n",
    "build a model with parameters of T4 & T6, which got the highest results in val_acc and in test accuracy over kaggle, so \n",
    "oversample and train for more than 10 epochs, and change the message_calculation_class = `ggnn`\n",
    "\n",
    "**setting**\n",
    "- sampling strategy is 1:2 , so that i don't overflood the data with repeated dummy samples.\n",
    "- one gcn layer with hidden_dim=32\n",
    "- defalut message_calculation_class = 'rgat'\n",
    "- one dense layer\n",
    "- hypers: adam\n",
    "- batch_size = 16\n",
    "> epoch = 20 , no early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76d28b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn data to data frame to easily duplicate the minority class later\n",
    "df = pd.DataFrame(training_set)\n",
    "X = df.iloc[:, : -1].values\n",
    "y = df.iloc[:, -1].values\n",
    " # create 9*duplicates , plus original will be 10\n",
    "class_oversample = df.loc[df[df[2]==1].index.repeat(9)]\n",
    "\n",
    "df_over = pd.concat([df, class_oversample])\n",
    "training_set_over = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edae305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_11 (TFOpLam  ()                  0           ['input_36[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 20)           10000       ['input_34[0][0]']               \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  ()                  0           ['tf.math.reduce_max_11[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_11 (GNN)                   (None, 32)           22720       ['embedding_11[0][0]',           \n",
      "                                                                  'input_35[0][0]',               \n",
      "                                                                  'input_36[0][0]',               \n",
      "                                                                  'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_10 (TFOpL  (None, 32)          0           ['gnn_11[0][0]',                 \n",
      " ambda)                                                           'input_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            33          ['tf.math.segment_mean_10[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,753\n",
      "Trainable params: 32,753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "params['message_calculation_class'] = 'rgat'\n",
    "params['num_heads'] = 16\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "# print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "# print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "# print('pred:', pred)\n",
    "\n",
    "model7 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f204cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.compile( optimizer = 'adam',\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a13d1473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/concat_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/concat_2:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgat_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgat/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgat/embedding_lookup_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgat/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradients/rgat/embedding_lookup_1_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330/1330 [==============================] - 71s 43ms/step - loss: 0.2301 - auc: 0.4920 - val_loss: 0.2274 - val_auc: 0.6327\n",
      "Epoch 2/20\n",
      "1330/1330 [==============================] - 55s 41ms/step - loss: 0.1869 - auc: 0.6367 - val_loss: 0.1992 - val_auc: 0.6661\n",
      "Epoch 3/20\n",
      "1330/1330 [==============================] - 58s 43ms/step - loss: 0.1827 - auc: 0.6655 - val_loss: 0.2004 - val_auc: 0.6913\n",
      "Epoch 4/20\n",
      "1330/1330 [==============================] - 55s 41ms/step - loss: 0.1799 - auc: 0.6819 - val_loss: 0.2044 - val_auc: 0.6817\n",
      "Epoch 5/20\n",
      "1330/1330 [==============================] - 58s 43ms/step - loss: 0.1766 - auc: 0.7106 - val_loss: 0.1947 - val_auc: 0.7173\n",
      "Epoch 6/20\n",
      "1330/1330 [==============================] - 55s 41ms/step - loss: 0.1745 - auc: 0.7259 - val_loss: 0.1914 - val_auc: 0.7213\n",
      "Epoch 7/20\n",
      "1330/1330 [==============================] - 57s 43ms/step - loss: 0.1743 - auc: 0.7238 - val_loss: 0.1938 - val_auc: 0.7270\n",
      "Epoch 8/20\n",
      "1330/1330 [==============================] - 55s 41ms/step - loss: 0.1707 - auc: 0.7421 - val_loss: 0.1882 - val_auc: 0.7366\n",
      "Epoch 9/20\n",
      "1330/1330 [==============================] - 55s 41ms/step - loss: 0.1711 - auc: 0.7404 - val_loss: 0.1934 - val_auc: 0.7412\n",
      "Epoch 10/20\n",
      "1330/1330 [==============================] - 60s 45ms/step - loss: 0.1694 - auc: 0.7466 - val_loss: 0.1856 - val_auc: 0.7355\n",
      "Epoch 11/20\n",
      "1330/1330 [==============================] - 55s 41ms/step - loss: 0.1687 - auc: 0.7512 - val_loss: 0.1885 - val_auc: 0.7496\n",
      "Epoch 12/20\n",
      "1330/1330 [==============================] - 60s 45ms/step - loss: 0.1674 - auc: 0.7571 - val_loss: 0.1921 - val_auc: 0.7337\n",
      "Epoch 13/20\n",
      "1330/1330 [==============================] - 54s 41ms/step - loss: 0.1675 - auc: 0.7605 - val_loss: 0.1993 - val_auc: 0.7066\n",
      "Epoch 14/20\n",
      "1330/1330 [==============================] - 58s 44ms/step - loss: 0.1662 - auc: 0.7658 - val_loss: 0.1817 - val_auc: 0.7470\n",
      "Epoch 15/20\n",
      "1330/1330 [==============================] - 56s 42ms/step - loss: 0.1670 - auc: 0.7668 - val_loss: 0.1869 - val_auc: 0.7728\n",
      "Epoch 16/20\n",
      "1330/1330 [==============================] - 57s 43ms/step - loss: 0.1663 - auc: 0.7665 - val_loss: 0.1857 - val_auc: 0.7544\n",
      "Epoch 17/20\n",
      "1330/1330 [==============================] - 56s 42ms/step - loss: 0.1640 - auc: 0.7722 - val_loss: 0.1903 - val_auc: 0.7689\n",
      "Epoch 18/20\n",
      "1330/1330 [==============================] - 56s 42ms/step - loss: 0.1629 - auc: 0.7777 - val_loss: 0.1808 - val_auc: 0.7550\n",
      "Epoch 19/20\n",
      "1330/1330 [==============================] - 63s 47ms/step - loss: 0.1631 - auc: 0.7765 - val_loss: 0.1889 - val_auc: 0.7524\n",
      "Epoch 20/20\n",
      "1330/1330 [==============================] - 59s 44ms/step - loss: 0.1618 - auc: 0.7813 - val_loss: 0.1825 - val_auc: 0.7739\n",
      "Wall time: 19min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_batchs = math.ceil(len(training_set_over) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "history = model7.fit(\n",
    "    gen_batch(\n",
    "        training_set_over, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=20,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation, \n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32b32119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets check the overfitting of the model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "714c39d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b160fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model7.predict(\n",
    "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
    ")\n",
    "\n",
    "y_pred = np.reshape(y_pred, -1)\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('T7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5463918",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"t8\"></a>\n",
    "\n",
    "# Trial 8:\n",
    "well, we have seen what overfitting looks like :D, it looks like T7. \n",
    "\n",
    "**settings** same as T7 but\n",
    "\n",
    "- make batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94206da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn data to data frame to easily duplicate the minority class later\n",
    "df = pd.DataFrame(training_set)\n",
    "X = df.iloc[:, : -1].values\n",
    "y = df.iloc[:, -1].values\n",
    " # create 9*duplicates , plus original will be 10\n",
    "class_oversample = df.loc[df[df[2]==1].index.repeat(9)]\n",
    "\n",
    "df_over = pd.concat([df, class_oversample])\n",
    "training_set_over = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72ea2379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_37 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_12 (TFOpLam  ()                  0           ['input_39[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 20)           10000       ['input_37[0][0]']               \n",
      "                                                                                                  \n",
      " input_38 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  ()                  0           ['tf.math.reduce_max_12[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " gnn_12 (GNN)                   (None, 32)           22464       ['embedding_12[0][0]',           \n",
      "                                                                  'input_38[0][0]',               \n",
      "                                                                  'input_39[0][0]',               \n",
      "                                                                  'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.segment_mean_11 (TFOpL  (None, 32)          0           ['gnn_12[0][0]',                 \n",
      " ambda)                                                           'input_39[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            33          ['tf.math.segment_mean_11[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,497\n",
      "Trainable params: 32,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "model8 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32919732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model8.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e293263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "665/665 [==============================] - 26s 39ms/step - loss: 0.2366 - auc: 0.4760 - val_loss: 0.2218 - val_auc: 0.6523\n",
      "Epoch 2/5\n",
      "665/665 [==============================] - 24s 35ms/step - loss: 0.1951 - auc: 0.6014 - val_loss: 0.1991 - val_auc: 0.6339\n",
      "Epoch 3/5\n",
      "665/665 [==============================] - 25s 38ms/step - loss: 0.1917 - auc: 0.6189 - val_loss: 0.2150 - val_auc: 0.6756\n",
      "Epoch 4/5\n",
      "665/665 [==============================] - 24s 36ms/step - loss: 0.1902 - auc: 0.6276 - val_loss: 0.2013 - val_auc: 0.6593\n",
      "Epoch 5/5\n",
      "665/665 [==============================] - 26s 40ms/step - loss: 0.1880 - auc: 0.6472 - val_loss: 0.2018 - val_auc: 0.6750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170d64c2ac0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "num_batchs = math.ceil(len(training_set_over) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model8.fit(\n",
    "    gen_batch(\n",
    "        training_set_over, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=5,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=16, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05698096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49969295, 0.49970955, 0.49963474, ..., 0.49972904, 0.4994854 ,\n",
       "       0.49964124], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(\n",
    "    gen_batch(testing_set, batch_size=32, shuffle=False)\n",
    ")\n",
    "\n",
    "y_pred = np.reshape(y_pred, -1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbde82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('T8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed5854",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"t9\"></a>\n",
    "\n",
    "# Trial 9:\n",
    "\n",
    "I want to try adding more than one gcn layer,\n",
    "\n",
    "**Status** Trial not completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2485a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.math import segment_mean\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import Input, Model\n",
    "# from tensorflow.keras.layers import Embedding, Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# from tf2_gnn.layers.gnn import GNN, GNNInput\n",
    "\n",
    "# data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# # the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "# edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "# node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "# embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# # number of graphs (number of samples)\n",
    "# num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "# gnn_input = GNNInput(\n",
    "#     node_features=embeded,\n",
    "#     adjacency_lists=(edge,),\n",
    "#     node_to_graph_map=node2graph, \n",
    "#     num_graphs=num_graph,\n",
    "# )\n",
    "\n",
    "# # https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "# params = GNN.get_default_hyperparameters()\n",
    "# params[\"hidden_dim\"] = 32\n",
    "# gnn_layer = GNN(params)\n",
    "# gnn_1 = GNN(params)\n",
    "# gnn_1 = gnn_layer(gnn_input)\n",
    "# gnn_out = gnn_1(params)\n",
    "# avg = segment_mean(\n",
    "#     data=gnn_out,\n",
    "#     segment_ids=node2graph\n",
    "# )\n",
    "# print('mean:', avg)\n",
    "\n",
    "# pred = Dense(1, activation='sigmoid')(avg)\n",
    "# print('pred:', pred)\n",
    "\n",
    "# model = Model(\n",
    "#     inputs={\n",
    "#         'data': data,\n",
    "#         'edges': edge,\n",
    "#         'node2grah': node2graph,\n",
    "#     },\n",
    "#     outputs=pred\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b1b36",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "<a class=\"anchor\" id=\"t10\"></a>\n",
    "\n",
    "# Trial 10:\n",
    "I have no more trial i'd like to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be13dd",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "<a class=\"anchor\" id=\"conc\"></a>\n",
    "\n",
    "Graph data is a big part of real problems, there is a lot of decisions to maake when it comes to training its model, how would the model handle the nodes, individually(attention mode) or dense agggregation to focus on all nodes together, or any other method.\n",
    "Another consideration is how big should the model be, number of gcn layers "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C3_npl_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
