{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca78291",
   "metadata": {
    "id": "POwDaceEsKCg"
   },
   "source": [
    "# CISC 873 Data Mining Competition #3\n",
    "name: Asmaa Qindeel\n",
    "\n",
    "\n",
    "Competition #3: https://www.kaggle.com/c/cisc-873-dm-f22-a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2645bc1",
   "metadata": {
    "id": "e_poxWuQsUYP"
   },
   "source": [
    "# TOPICS:\n",
    "* [Questions & Answers](#Questions)\n",
    "* [Problem and Protocol](#intro)\n",
    "\n",
    "**Code Part**\n",
    "* [Preparing workspace: gathering info about the data](#pre)\n",
    "    * [Data Balance](#balance)\n",
    "* [Trail 1: no preprocessing/ tfidf/ XGBoost](#t1)\n",
    "* [Trail 2: clean text/ tfidf/ xgboost](#t2)\n",
    "* [Trail 3: / countvectorizor/ Logistic Regression](#t3)\n",
    "* [Trail 4: best hypers from above / charachter level vectorizor](#t4)\n",
    "* [Trail 5: Random Search with XGboost](#t5)\n",
    "    * [5.2](#t5.2)\n",
    "* [6](#6)\n",
    "\n",
    "\n",
    "[**Conclusion**](#conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d721248",
   "metadata": {
    "id": "fex7AHnxsXuN"
   },
   "source": [
    "# Questions:  \n",
    "<a class=\"anchor\" id=\"Questions\"></a>\n",
    "\n",
    "1. What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "    - Character n-gram works on the character level, while Word n-gram operates on the word level which is why it suffers more from the OOV issue.\n",
    "2. What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "    - they are both steps in preprocessing text, but stop word removes words that repeat so much that they are useless, while stemming doesn't remove words, but rather modify them. Stemming is language-dependant, e.g. it depends on the rules of each language to stem words of it. stop words can just be found with some frequency analysis, they are different for each language of course, but they don't depend on the rules of the language itself.\n",
    "3. Is tokenization techniques language dependent? Why?\n",
    "    - Yes, you nedd to know the rules of the language to understand which word is more useful(the weights of each word in the symantics of the language) i.e. in Arabic one letter attached to a wrod/verb can change its value greatly, making it more useful.\n",
    "4. What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "    - Count focuses on the frequency of the word, it doesn't indicate the importance of the word through the whole dataset, while tfidf takes into account the frequency of the document it self, this makes tfidf reduce feature dimention by focusing on the higher frequency documents.\n",
    "    - Using all possible n-grams is computationaly impossible, a very high cost. The best n-gram depends on your model, you can narrow the search down to some values of n depending on your understanding of the model, and use tuning to get the best n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fab6ca",
   "metadata": {
    "id": "7mo66ko2sgbR"
   },
   "source": [
    "# Problem Formulation:<a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "https://www.kaggle.com/c/cisc-873-dm-f22-a3\n",
    "\n",
    "The problem is an NLP Binary classification, to get the real news headlines out of the fake news. The input space is a st of 60k news headline, with target 0/1, zero being fake news, and 1 being real news. The data is row as we'll see.\n",
    "In this project i'll explore text preprocessing, word/character vectorizer, n-gram effect, all with the use of a pipeline. The object is to experience NLP, tune text preprocessing hyperparameters. \n",
    "\n",
    "The ideal solution i think will have no preprocessing(no data cleaning), because the data is headlines. News headlines are supposed to be clean, short text. The Count vectorizor vs the tfidf i think may be close for the same reason(news headlines). The best model i'm thinking will be linear (LogReg, SVC,....)\n",
    "\n",
    "\n",
    "Challenges \n",
    "\n",
    "Metric used in this problem is roc_auc.\n",
    "\n",
    "## The Impact:\n",
    "for Me, learning. for the social media companies==> a better society. Blocking the fake provocative news will have an impact on reducing conflict in social gatherings. I have seen people fighting over the stupidest little piece of news, even tending to violence. Increase awareness, with the absence of fake news people will tend more to the real news, awareness of their reality is the first step in changing it.\n",
    "\n",
    "\n",
    "\n",
    "# Experimental Protocol:\n",
    "\n",
    "Using pipeline to contain the preprocessors(vectorizor) and the model.\n",
    "\n",
    "Use Bag of words(BoW) as text processing technique\n",
    "\n",
    "I will first check my hypothesis about the data, by trying cleaning and not cleaning text, with fixed model and vectorizor. Then i'll change the vectorizor to see which is better(word vs character victorizer) but for both it will be the measure of tf-idf for the words. Then i'll change the model between XGBoost and LogesticRegression.\n",
    "\n",
    "For Preprocessing i'll use stemming once and embedding once.\n",
    "\n",
    "I also use Random search with predifined Validation Set, or grid search when the grid is small enough. Metric is `'roc_auc'` will be the decision maker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64078d8b",
   "metadata": {
    "id": "f808a0ed"
   },
   "source": [
    "# References\n",
    "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc.\n",
    "\n",
    "873_nlp lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7eecf0",
   "metadata": {
    "executionInfo": {
     "elapsed": 2869,
     "status": "ok",
     "timestamp": 1647248501884,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "4ec5d4e8"
   },
   "outputs": [],
   "source": [
    "# libs for text preprocessing\n",
    "import re\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f0753c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1647248623337,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "ae19d4bb"
   },
   "outputs": [],
   "source": [
    "#libs for modeling\n",
    "\n",
    "# NLTK tools\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vectorizor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# building model and tunign tools\n",
    "from sklearn.model_selection import train_test_split,  PredefinedSplit\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "#import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5fef2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2050,
     "status": "ok",
     "timestamp": 1647248539031,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "644a065b",
    "outputId": "6b505b5a-4ea1-4e7a-b4ea-f83033742757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      60000 non-null  int64 \n",
      " 1   text    60000 non-null  object\n",
      " 2   label   60000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "\n",
    "train_file = pd.read_csv('train.csv' , header = 0 )#, index_col=0)\n",
    "test_file = pd.read_csv('test.csv' , header = 0 )#, index_col=0)\n",
    "\n",
    "#train_file.head()\n",
    "train_file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70a3d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647248543545,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "55e0f01b",
    "outputId": "65e0ab9f-a40b-42cf-d8d7-d294ff4a2bb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       0\n",
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# always make a copy \n",
    "df = train_file.copy()\n",
    "df_test = test_file.copy()\n",
    "# check the count of null values\n",
    "null_count = df.isnull().sum().sort_values(ascending = False)\n",
    "null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5db68aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1647182981575,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "834abe2f",
    "outputId": "5fb33769-4dae-4654-8d90-0264dbfc1875"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265723</td>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284269</td>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207715</td>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551106</td>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8584</td>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>117912</td>\n",
       "      <td>In the 1920鈥檚, Hitler was forbidden to address...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>213064</td>\n",
       "      <td>Nerd Wins Scrabble with word you've never hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>398923</td>\n",
       "      <td>Why 95.8% of Female Newscasters Have the Same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>314798</td>\n",
       "      <td>Donald Trump Says He'll Do This If More 'Inapp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20243</td>\n",
       "      <td>5 crazy facts about Lamborghini's outrageous e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  label\n",
       "0  265723  A group of friends began to volunteer at a hom...      0\n",
       "1  284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
       "2  207715  In 1961, Goodyear released a kit that allows P...      0\n",
       "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "4    8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n",
       "5  117912  In the 1920鈥檚, Hitler was forbidden to address...      0\n",
       "6  213064  Nerd Wins Scrabble with word you've never hear...      0\n",
       "7  398923  Why 95.8% of Female Newscasters Have the Same ...      1\n",
       "8  314798  Donald Trump Says He'll Do This If More 'Inapp...      0\n",
       "9   20243  5 crazy facts about Lamborghini's outrageous e...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca1e411",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1647182981575,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "a726a2a1",
    "outputId": "84221517-a0a9-45a9-f521-a725b760d36b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32172\n",
       "1    27596\n",
       "2      232\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the target distribution\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17cada5",
   "metadata": {
    "id": "b9afb7c0"
   },
   "source": [
    "There shouldn't be a target 2 , \n",
    "\n",
    "but 232 of 60000  is a small ratio, i'll consider it wrong data and drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7ce6e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1647248549496,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "7f644518",
    "outputId": "c55356c3-5628-4b53-db71-a0a32cb3b5fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32172\n",
       "1    27596\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the rows where label=2 \n",
    "#df[df.label == 2]\n",
    "df.drop(df[df.label == 2].index, axis = 0, inplace=True)\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f518d",
   "metadata": {
    "id": "651431da"
   },
   "source": [
    "# Trial 1:\n",
    "<a class=\"anchor\" id=\"t1\"></a>\n",
    "## No Preprocessing :\n",
    "\n",
    "just input the data as it is into the model, to get a feel of the benefit of preprocessing. Use random search to search for hyperparameters of the vectorizer and the model\n",
    "- model: XGBoost\n",
    "- feature extraction: TfidfVectorizer, on word level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fc481b",
   "metadata": {
    "id": "n-c__ygWzLU3"
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y , test_size = 0.2 , stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de23f391",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1647186340537,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "d1e6f6b6",
    "outputId": "b8383de6-081e-4c81-ecb1-80808c70a42c"
   },
   "outputs": [],
   "source": [
    "#create the pipeline\n",
    "full_pipe = Pipeline(steps=[\n",
    "        # analyzer = 'word' means word-level vectorizer.\n",
    "    ('vectorizor', TfidfVectorizer(stop_words='english', analyzer='word')) , \n",
    "    ('my_model', XGBClassifier(use_label_encoder=False, verbosity = 0)) \n",
    "])\n",
    "\n",
    "#define parameter grid for the tuning\n",
    "param_grid = {\n",
    "    #parameters of XGBoost\n",
    "    'my_model__max_depth':[11, 13, 17],\n",
    "    'my_model__learning_rate':[0.1],\n",
    "    \n",
    "    #parameters for the Vectorizer\n",
    "    'vectorizor__ngram_range': [(1, 2), (1, 3)],\n",
    "    'vectorizor__max_df': np.arange(0.3, 0.8),\n",
    "    \"vectorizor__min_df\": np.arange(5, 50)\n",
    "}\n",
    "\n",
    "#create predefined split\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "pre_defined_split = PredefinedSplit(test_fold = split_index )\n",
    "\n",
    "#cast y as int for the XGBoost\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be047ac2",
   "metadata": {
    "id": "L2zag53e9QmT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of the cv: 0.8386354379650266\n",
      "best Hyper set: {'vectorizor__ngram_range': (1, 2), 'vectorizor__min_df': 23, 'vectorizor__max_df': 0.3, 'my_model__max_depth': 17, 'my_model__learning_rate': 0.1}\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipe, param_grid, cv = pre_defined_split , scoring=\"roc_auc\", n_iter=15)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print('best score of the cv:', random_search.best_score_)\n",
    "print('best Hyper set:', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07590441",
   "metadata": {},
   "source": [
    "**model performance:**\n",
    "\n",
    "best score of the cv: 0.8365894860912074\n",
    "\n",
    "\n",
    "`best Hyper set: {'vectorizor__ngram_range': (1, 3), 'vectorizor__min_df': 34, 'vectorizor__max_df': 0.3, 'my_model__max_depth': 17, 'my_model__learning_rate': 0.1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8262c8c",
   "metadata": {
    "id": "sViXZmVX18bO"
   },
   "outputs": [],
   "source": [
    "#predict for the test file data\n",
    "# and save to file\n",
    "y_out = random_search.predict_proba(df_test['text'])\n",
    "\n",
    "dummy = pd.DataFrame({'id': df_test['id'],'label': y_out[:,1]})\n",
    "dummy.to_csv(\"nopre_T1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517aa5e0",
   "metadata": {},
   "source": [
    "# Trial 2: \n",
    "<a class=\"anchor\" id=\"t2\"></a>\n",
    "\n",
    "0.83 with no preprocessing, that was pretty good.\n",
    "\n",
    "now lets experience some simple preprocessing. I'll clean the text to create a WoB(word of bags); remove single letters remove any special characters, leave only the alphabet characters and the numbers(i thought it could be a good factor in recognizing fake and real news), remove any other shapes like tags: <>. Use \n",
    "Use best_parameters from T_1 for the model, and search parameters of the vectorizer with grid search with validation set.\n",
    "\n",
    "**setting:**\n",
    "\n",
    "- model: Use the best settings of the model from Trial_1\n",
    "- vectorizer: word-level vectorizer\n",
    "- tuning: a grid search this time with narrwing down the search space to become closer to the results of T1, for faster convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ec661e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1647248767655,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "bMRId51uvy8p",
    "outputId": "912c277a-13a9-46b5-f93b-31a9d9cb4ec5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asmaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asmaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download the package for tokenization and stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# get the stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# this piece of function was edited from the original in the 873_nlp lab\n",
    "def clean_text(text):\n",
    "##plan:\n",
    "## define the things to remove as RegEx\n",
    "## use re.sub function to replace them in text\n",
    "##stem \n",
    "\n",
    "#   1. define the whitespaces\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "#   2. define the tags\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "#   3. define all that is not english alphabet or numbers   \n",
    "    RE_ASCII = re.compile(r\"[^A-Za-z1-9 ]\", re.IGNORECASE)\n",
    "#   4. define single characters\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "\n",
    "    # now replace them in the text\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    #tokenize the text (separate it into words)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    #lower cases\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    #stem the words\n",
    "    words_filtered = [\n",
    "        stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "    ]\n",
    "\n",
    "    #put the words back together\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22f08c1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27784,
     "status": "ok",
     "timestamp": 1647186290219,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "de99d987",
    "outputId": "a24b2337-d45a-4cd0-efd7-26a83ae3d91f"
   },
   "outputs": [],
   "source": [
    "#first clean the train file\n",
    "# copy the text column to a new column\n",
    "df['clean_txt'] = df['text']\n",
    "# remap the new column with the clean_text function\n",
    "df['clean_txt'] = df['clean_txt'].map(\n",
    "    lambda x: clean_text(x ) if isinstance(x, str) else x\n",
    ")\n",
    "#also clean the test file\n",
    "df_test['clean_txt'] = df_test['text']\n",
    "df_test['clean_txt'] = df_test['clean_txt'].map(\n",
    "    lambda x: clean_text(x ) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2accf35",
   "metadata": {
    "id": "f6861156",
    "outputId": "86ebfb8a-c5fd-48ab-cbd6-c19bf8c8af7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265723</td>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "      <td>group friend began volunt homeless shelter nei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284269</td>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "      <td>british prime minist theresa may nerv attack f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207715</td>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "      <td>1961 goodyear releas kit allow ps2s brought he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551106</td>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "      <td>happi birthday bob barker price right host lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8584</td>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>obama nation innoc cop unarm young black men d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  label  \\\n",
       "0  265723  A group of friends began to volunteer at a hom...      0   \n",
       "1  284269  British Prime Minister @Theresa_May on Nerve A...      0   \n",
       "2  207715  In 1961, Goodyear released a kit that allows P...      0   \n",
       "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0   \n",
       "4    8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0   \n",
       "\n",
       "                                           clean_txt  \n",
       "0  group friend began volunt homeless shelter nei...  \n",
       "1  british prime minist theresa may nerv attack f...  \n",
       "2  1961 goodyear releas kit allow ps2s brought he...  \n",
       "3  happi birthday bob barker price right host lik...  \n",
       "4  obama nation innoc cop unarm young black men d...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5467a867",
   "metadata": {
    "id": "n-c__ygWzLU3"
   },
   "outputs": [],
   "source": [
    "X = df['clean_txt']\n",
    "y = df['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y , test_size = 0.3 , stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c4967b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1647186340537,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "d1e6f6b6",
    "outputId": "b8383de6-081e-4c81-ecb1-80808c70a42c"
   },
   "outputs": [],
   "source": [
    "#create the pipeline with tfidf vectorizer and model:XGBoost\n",
    "\n",
    "full_pipe = Pipeline(steps=[\n",
    "    # analyzer = 'word' means word-level vectorizer.\n",
    "    ('vectorizor', TfidfVectorizer(stop_words='english', analyzer='word')) , \n",
    "    ('my_model', XGBClassifier(use_label_encoder=False, verbosity = 0)) \n",
    "])\n",
    "\n",
    "#define parameter grid for the tuning\n",
    "param_grid = {\n",
    "    #parameters of XGBoost\n",
    "    #use the best settings from Trial_1\n",
    "    'my_model__max_depth':[17],\n",
    "    'my_model__learning_rate':[0.1],\n",
    "  \n",
    "    'vectorizor__ngram_range': [(1, 3)],\n",
    "    'vectorizor__max_df': np.arange(0.3, 0.8),\n",
    "    \"vectorizor__min_df\": np.arange(10, 47) #best min in Trial_1 was 34, \n",
    "                        # so i narrowed it down a little bit from 100. \n",
    "}\n",
    "\n",
    "# define the predifened validation set\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "pre_defined_split = PredefinedSplit(test_fold = split_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9627efc",
   "metadata": {
    "id": "L2zag53e9QmT"
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    full_pipe, param_grid, cv = pre_defined_split , scoring=\"roc_auc\" )\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print('best score of the cv:', grid_search.best_score_)\n",
    "print('best Hyper set:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc631685",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "best score of the cv: 0.8245970148910057\n",
    "\n",
    "`best Hyper set: {'my_model__learning_rate': 0.1, 'my_model__max_depth': 17, 'vectorizor__max_df': 0.3, 'vectorizor__min_df': 13, 'vectorizor__ngram_range': (1, 3)}\n",
    "Wall time: 6min 42s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e0c7518",
   "metadata": {
    "id": "sViXZmVX18bO"
   },
   "outputs": [],
   "source": [
    "#predict for the test file data\n",
    "# and save to file\n",
    "y_out = random_search.predict_proba(df_test['clean_txt'])\n",
    "\n",
    "dummy = pd.DataFrame({'id': df_test['id'],'label': y_out[:,1]})\n",
    "dummy.to_csv(\"clean_xgboost_T2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08f9d3",
   "metadata": {
    "id": "OBFqwoQC5qEz"
   },
   "source": [
    "# Trial 3:\n",
    "<a class=\"anchor\" id=\"t3\"></a>\n",
    "\n",
    "from trial 1 and 2 it is actually better not to clean the data?! there goes the logic.\n",
    "\n",
    "Now i'll focus on tuning the model and vectorizer and try another preprocessing afterwards. so i'll use data, and tune for the vectorizor type (word,char),\n",
    "\n",
    "**new setting:**\n",
    "- preprocessing: none\n",
    "- model: XGBoost\n",
    "- Tuning: grid search to tune the vectorizer type (word/char)\n",
    "- vectorizer: char level tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2573f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "# make the test size 0.3 because we have plenty of data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y , test_size = 0.3 , stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "709a1ad2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 551304,
     "status": "ok",
     "timestamp": 1647187367286,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "SWECDbLk5o7P",
    "outputId": "b4d8f2a0-85cb-4fee-9362-73827776392e"
   },
   "outputs": [],
   "source": [
    "#create the pipeline\n",
    "full_pipe = Pipeline(steps=[\n",
    "    ('vectorizor', TfidfVectorizer(stop_words='english')) , \n",
    "    ('my_model', XGBClassifier(use_label_encoder=False, verbosity = 0)) \n",
    "])\n",
    "\n",
    "#define parameter grid for the tuning\n",
    "param_grid = {\n",
    "    #parameters of XGBoost\n",
    "    #use the best settings from Trial_1 \n",
    "    'my_model__max_depth':[17],\n",
    "    'my_model__learning_rate':[0.1],\n",
    "  \n",
    "    'vectorizor__analyzer': ['char'],  # ,'char_wb'],\n",
    "    'vectorizor__ngram_range': [(3,5)], #[(3,5), (7,11)],\n",
    "    'vectorizor__max_df': [0.3],  # np.arange(0.3, 0.5, 0.1),\n",
    "    \"vectorizor__min_df\": [23] #np.arange(11, 23) #best min in last trial\n",
    "}\n",
    "\n",
    "# define the predifened validation set\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "pre_defined_split = PredefinedSplit(test_fold = split_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e578db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of the cv: 0.9251783918759096\n",
      "best Hyper set: {'my_model__learning_rate': 0.1, 'my_model__max_depth': 17, 'vectorizor__analyzer': 'char', 'vectorizor__max_df': 0.3, 'vectorizor__min_df': 23, 'vectorizor__ngram_range': (3, 5)}\n",
      "Wall time: 9min 34s\n"
     ]
    }
   ],
   "source": [
    "# define grid search and feed it the pipeline\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipe, param_grid, cv = pre_defined_split , scoring=\"roc_auc\" )\n",
    "#fit the search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print('best score of the cv:', grid_search.best_score_)\n",
    "print('best Hyper set:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8b310",
   "metadata": {},
   "source": [
    "**some of the ngram i tried, each at a time so the grid ends fast:**\n",
    "\n",
    "\n",
    "- ngram (11,13):best score of the cv: 0.7335391881465831\n",
    "- ngram (9,9): best score of the cv: 0.7914074548484631,  Wall time: 9min 34s\n",
    "- ngram (7,7): best score of the cv: 0.84\n",
    "- ngram (5,5): best score of the cv: 0.8918609987762567,  Wall time: 9min 34s\n",
    "    - ngram (3,5): best score of the cv: 0.9251783918759096, Wall time: 9min 34s\n",
    "    - ngram (3,4): best score of the cv: 0.9235048800817051, Wall time: 6min 19s\n",
    "- ngram (3,3): best score of the cv: 0.9169495846445554,  Wall time: 9min 34s\n",
    "- ngram (2,2): best score of the cv: 0.9033028582996028,  Wall time: 9min 34s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78ad3012",
   "metadata": {
    "id": "EnoHvhGz5o7Z"
   },
   "outputs": [],
   "source": [
    "#predict for the test file data\n",
    "# and save to file\n",
    "y_out = grid_search.predict_proba(df_test['text'])\n",
    "\n",
    "dummy = pd.DataFrame({'id': df_test['id'],'label': y_out[:,1]})\n",
    "dummy.to_csv(\"char_vector_T3_b.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24e8fb",
   "metadata": {
    "id": "OBFqwoQC5qEz"
   },
   "source": [
    "# Trial 4:\n",
    "<a class=\"anchor\" id=\"t4\"></a>\n",
    "\n",
    "Trial 3 was overfitting, it resulted > 0.92, with `ngram_range` (3,5), but failed in the test set on kaggle. \n",
    "\n",
    "I should've tested it with a separate test set and see its f1-score.\n",
    "That is becasuse the character level analyzer doesn't capture a crucial information in our problem, i.e. a set of letters 'sem' for example doesn't have the same effect as a whole word like 'assemple' the repetition of it is different. The model probably learned the pattern of the letters in the words, nothing more, which is pointless unless this is a translation problem.\n",
    "\n",
    "**next plan:**\n",
    "\n",
    "one more check of the character vectorizer. I want to try the option `char_wb` in the analyzer.\n",
    "with the same set of parameters as Trial_3: \n",
    "**T_3 setting:**\n",
    "- preprocessing: none\n",
    "- model: XGBoost\n",
    "- Tuning: grid search to tune the vectorizer type (word/char)\n",
    "- vectorizer: char_wb level tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891a2785",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 551304,
     "status": "ok",
     "timestamp": 1647187367286,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "SWECDbLk5o7P",
    "outputId": "b4d8f2a0-85cb-4fee-9362-73827776392e"
   },
   "outputs": [],
   "source": [
    "#create the pipeline\n",
    "full_pipe = Pipeline(steps=[\n",
    "    #'char_wb' creates character n-grams only from text inside word boundaries\n",
    "    ('vectorizor', TfidfVectorizer(stop_words='english')) ,\n",
    "    ('my_model', XGBClassifier(use_label_encoder=False, verbosity = 0)) \n",
    "])\n",
    "\n",
    "#define parameter grid for the tuning\n",
    "param_grid = {\n",
    "    #parameters of XGBoost\n",
    "    #use the best settings from Trial_1 \n",
    "    'my_model__max_depth':[17],\n",
    "    'my_model__learning_rate':[0.1],\n",
    "  \n",
    "     #best combination from Trial 3\n",
    "    'vectorizor__analyzer': ['char_wb'], #char_wb means characters only inside words\n",
    "    'vectorizor__ngram_range': [(3,4)], #[(3,5), (7,11)],\n",
    "    'vectorizor__max_df': [0.3],  \n",
    "    \"vectorizor__min_df\": [23] \n",
    "}\n",
    "\n",
    "# define the predifened validation set\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "pre_defined_split = PredefinedSplit(test_fold = split_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8ba3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of the cv: 0.9252480904772987\n",
      "best Hyper set: {'my_model__learning_rate': 0.1, 'my_model__max_depth': 17, 'vectorizor__analyzer': 'char_wb', 'vectorizor__max_df': 0.3, 'vectorizor__min_df': 23, 'vectorizor__ngram_range': (3, 4)}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search model and feed it the pipelene\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipe, param_grid, cv = pre_defined_split , scoring=\"roc_auc\" )\n",
    "#train the search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print('best score of the cv:', grid_search.best_score_)\n",
    "print('best Hyper set:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725049bc",
   "metadata": {},
   "source": [
    "**some of the ngram i tried with `char_wb`, each at a time so the grid ends fast:**\n",
    "\n",
    "\n",
    "- ngram (7,7):  best score of the cv: 0.8850190331721215, Wall time: 54.6 s\n",
    "- ngram (5,7):  best score of the cv: 0.9038577714014062, Wall time: 3min 25s\n",
    "- ngram (3,4): best score of the cv: 0.92297878103903, Wall time: 4min 47s\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b567eea",
   "metadata": {
    "id": "EnoHvhGz5o7Z"
   },
   "outputs": [],
   "source": [
    "#predict for the test file data\n",
    "# and save to file\n",
    "y_out = grid_search.predict_proba(df_test['text'])\n",
    "\n",
    "dummy = pd.DataFrame({'id': df_test['id'],'label': y_out[:,1]})\n",
    "dummy.to_csv(\"char_wb_vector_T4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee45f62",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "# Trial 5:\n",
    "<a class=\"anchor\" id=\"t5\"></a>\n",
    "`char_wb ngram (3,4): best score of the cv: 0.92297878103903`\n",
    "although this seems to be a good result, i doubt it will perform will on test data, and i was right, not better than previous models at least, on kaggle results.\n",
    "\n",
    "now the **best settings** so far, is with:\n",
    "\n",
    "- no text cleaning\n",
    "- word level vectorizer\n",
    "- ngram_range(1,3)\n",
    "- max_df: 0.3 \n",
    "- min_df: 23\n",
    "\n",
    "so the **plan** is to use that setting with a new model, because the XGBoost tends to overfit, because it is basically a random tree with so much splits.\n",
    "I'll also use testing set, a 0.2 of the data.\n",
    "- model: LogisticRegression\n",
    "- tuning: grid search, tune the logistic regression\n",
    "- vectorizer: word level, ngram(1,3) with best parameters from before\n",
    "- Testing: separated testing set, so i can check what happened in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e021fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df['text']\n",
    "y1 = df['label']\n",
    "# make the test size 0.3 because we have plenty of data\n",
    "X, X_ts, y, y_ts = train_test_split( X1, y1 , test_size = 0.2 , stratify = y1, random_state = 42) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y , test_size = 0.2 , stratify = y, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29415e03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43215,
     "status": "ok",
     "timestamp": 1647188733051,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "xQxbDd8YFQHe",
    "outputId": "9fbb86fc-4ac2-4805-8e82-9ae56f8dd405"
   },
   "outputs": [],
   "source": [
    "#create the pipeline\n",
    "full_pipe = Pipeline(steps=[\n",
    "    ('vectorizor', TfidfVectorizer(stop_words='english')) , \n",
    "    ('my_model', LogisticRegression(verbose=0)) \n",
    "])\n",
    "\n",
    "#define parameter grid for the tuning\n",
    "param_grid = {\n",
    "     #hypers of the Logistic Reg\n",
    "        #use the best settings from Trial_1 \n",
    "    'my_model__penalty': [None, 'l1', 'l2'],\n",
    "    'my_model__C':[1.0, 0.8],\n",
    "    'my_model__max_iter': [10000], # so the model can converge\n",
    "   \n",
    "     #best combination from Trial 2\n",
    "    'vectorizor__analyzer': ['word'],\n",
    "    'vectorizor__ngram_range': [(1,3)],  \n",
    "    'vectorizor__max_df': [0.3],  \n",
    "    \"vectorizor__min_df\": [23] \n",
    "}\n",
    "\n",
    "# define the predifened validation set\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "pre_defined_split = PredefinedSplit(test_fold = split_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4db6240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "4 fits failed out of a total of 6.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.87619542        nan        nan 0.87585703]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of the cv: 0.8761954196552159\n",
      "best Hyper set: {'my_model__C': 1.0, 'my_model__max_iter': 10000, 'my_model__penalty': 'l2', 'vectorizor__analyzer': 'word', 'vectorizor__max_df': 0.3, 'vectorizor__min_df': 23, 'vectorizor__ngram_range': (1, 3)}\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    full_pipe, param_grid, cv = pre_defined_split , scoring=\"roc_auc\", verbose=0 )\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print('best score of the cv:', grid_search.best_score_)\n",
    "print('best Hyper set:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b52c7e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81      6435\n",
      "           1       0.77      0.79      0.78      5519\n",
      "\n",
      "    accuracy                           0.80     11954\n",
      "   macro avg       0.79      0.79      0.79     11954\n",
      "weighted avg       0.80      0.80      0.80     11954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing and checking with more metrics\n",
    "y_hat = grid_search.predict(X_ts)\n",
    "report = classification_report(y_ts, y_hat)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83101e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict for the test file data\n",
    "# and save to file\n",
    "y_out = grid_search.predict_proba(df_test['text'])\n",
    "\n",
    "dummy = pd.DataFrame({'id': df_test['id'],'label': y_out[:,1]})\n",
    "dummy.to_csv(\"logistic_T5_noclean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcc907",
   "metadata": {
    "id": "hOwTdWl0FrOe"
   },
   "source": [
    "# Trial 5.2:\n",
    "<a class=\"anchor\" id=\"t5.2\"></a>\n",
    "Trial_4 macro f1-score is 0.79 with logistic regression and word-level vectorizer.\n",
    "I think this is good enough, but lets try again and see if it gets better if we reversed to char-level vectorizer.\n",
    "\n",
    "Trail_5 setting \n",
    "again with character level \n",
    "- no text cleaning\n",
    "    - 'vectorizor__analyzer': ['char_wb'],\n",
    "    - 'vectorizor__ngram_range': [(3,4)],\n",
    "    - 'vectorizor__max_df': [0.3],  \n",
    "    - \"vectorizor__min_df\": [23] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3724899",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df['text']\n",
    "y1 = df['label']\n",
    "# make the test size 0.2 because we have plenty of data\n",
    "X, X_ts, y, y_ts = train_test_split( X1, y1 , test_size = 0.2 , stratify = y1, random_state = 42) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y , test_size = 0.2 , stratify = y, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37fad201",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43215,
     "status": "ok",
     "timestamp": 1647188733051,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "xQxbDd8YFQHe",
    "outputId": "9fbb86fc-4ac2-4805-8e82-9ae56f8dd405"
   },
   "outputs": [],
   "source": [
    "#create the pipeline\n",
    "full_pipe = Pipeline(steps=[\n",
    "    #'char_wb' creates character n-grams only from text inside word boundaries\n",
    "    ('vectorizor', TfidfVectorizer(stop_words='english')) , \n",
    "    ('my_model', LogisticRegression(verbose=0)) \n",
    "])\n",
    "\n",
    "#define parameter grid for the tuning\n",
    "param_grid = {\n",
    "     #hypers of the Logistic Reg\n",
    "        #use the best settings from Trial_1 \n",
    "    'my_model__penalty': [None, 'l2'],\n",
    "    'my_model__C':[1.0, 0.8],\n",
    "    'my_model__max_iter': [10000], # so the model can converge\n",
    "   \n",
    "     #best combination from Trial 3\n",
    "    'vectorizor__analyzer': ['char_wb'],\n",
    "    'vectorizor__ngram_range': [(3,4)], #[(3,5), (7,11)],\n",
    "    'vectorizor__max_df': [0.3],  \n",
    "    \"vectorizor__min_df\": [23] \n",
    "}\n",
    "\n",
    "# define the predifened validation set\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "pre_defined_split = PredefinedSplit(test_fold = split_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b173b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2 fits failed out of a total of 4.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.92067544        nan 0.91904893]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of the cv: 0.9206754363039752\n",
      "best Hyper set: {'my_model__C': 1.0, 'my_model__max_iter': 10000, 'my_model__penalty': 'l2', 'vectorizor__analyzer': 'char_wb', 'vectorizor__max_df': 0.3, 'vectorizor__min_df': 23, 'vectorizor__ngram_range': (3, 4)}\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#show how much time this will take\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipe, param_grid, cv = pre_defined_split , scoring=\"roc_auc\", verbose=0 )\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print('best score of the cv:', grid_search.best_score_)\n",
    "print('best Hyper set:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203013c",
   "metadata": {},
   "source": [
    "best score of the cv: 0.9206754363039752\n",
    "\n",
    "`best Hyper set: {'my_model__C': 1.0, 'my_model__max_iter': 10000, 'my_model__penalty': 'l2', 'vectorizor__analyzer': 'char_wb', 'vectorizor__max_df': 0.3, 'vectorizor__min_df': 23, 'vectorizor__ngram_range': (3, 4)}\n",
    "Wall time: 1min 12s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e405243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      6435\n",
      "           1       0.82      0.85      0.84      5519\n",
      "\n",
      "    accuracy                           0.85     11954\n",
      "   macro avg       0.85      0.85      0.85     11954\n",
      "weighted avg       0.85      0.85      0.85     11954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing with test set\n",
    "y_hat = grid_search.predict(X_ts)\n",
    "report = classification_report(y_ts, y_hat)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36322be",
   "metadata": {},
   "source": [
    "![image](https://sayingimages.com/wp-content/uploads/jackie-chan-wait-what-meme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01904a02",
   "metadata": {},
   "source": [
    "## 5.2 summary\n",
    "soo\n",
    "- model: logistic with\n",
    "- char_wb tfidf vectorizer, ngram(3,4), \n",
    "- AAAAAAnd no preprocessing, NONE\n",
    "performed better than all above?! \n",
    "\n",
    "`roc_auc` = 0.92 \n",
    "\n",
    "`macro f1-score` = 0.85 for both classes\n",
    "\n",
    "\n",
    "Performed better than Trial_2 wich preprocessing and word-level vectorizer?!\n",
    "WHAAAT?!\n",
    "\n",
    "That doesn't make sense to me, word level vectorizer should be better in this problem, why is it not?!\n",
    ">I thought maybe char_wb with ngram(3,4) could just be resulting the whole words again, so i tried word-level with ngram(1,1) but it got less roc_auc score it got 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dbb40e2d",
   "metadata": {
    "id": "0EX8cLXKGOGN"
   },
   "outputs": [],
   "source": [
    "#predict for the test file data\n",
    "# and save to file\n",
    "y_out = grid_search.predict_proba(df_test['text'])\n",
    "\n",
    "dummy = pd.DataFrame({'id': df_test['id'],'label': y_out[:,1]})\n",
    "dummy.to_csv(\"logistic_T5_b_char.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2af243",
   "metadata": {},
   "source": [
    "# Trial 6: \n",
    "<a class=\"anchor\" id=\"t6\"></a>\n",
    "That last Trial_5.2 rendered me quizzical, but i still have one more technique i want to explore, cleaning/preprocessing without stemming, as a simple Word Embedding. Acomplex word embedding is something like word2vec.\n",
    "\n",
    "**Setting:**\n",
    "- Preprocessing: Word embedding\n",
    "- model: logistic regression\n",
    "- tuning: grid search\n",
    "- vectorizer: char_wb level tfidf vectorizer with ngram(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ae67549",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1647248767655,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "bMRId51uvy8p",
    "outputId": "912c277a-13a9-46b5-f93b-31a9d9cb4ec5"
   },
   "outputs": [],
   "source": [
    "def word_embedding(text ):\n",
    "##plan:\n",
    "## define the things to remove as RegEx\n",
    "## use re.sub function to replace them in text\n",
    "## \n",
    "\n",
    "#   1. define the whitespaces\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "#   2. define the tags\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "#   3. define all that is not english alphabet of punctuation  \n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "#   4. define single characters\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    # now replace them in the text\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    #tokenize the text (separate it into words)    \n",
    "    word_tokens = word_tokenize(text)\n",
    "  #  print(word_tokens)\n",
    "    #put the words back together\n",
    "    text_clean = \" \".join(word_tokens)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04fa9d99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27784,
     "status": "ok",
     "timestamp": 1647186290219,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "de99d987",
    "outputId": "a24b2337-d45a-4cd0-efd7-26a83ae3d91f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#clean the train file\n",
    "df['clean_txt2'] = df['text']\n",
    "df['clean_txt2'] = df['clean_txt2'].map(\n",
    "    lambda x: word_embedding(x) if isinstance(x, str) else x\n",
    ")\n",
    "# also clean the test file\n",
    "df_test['clean_txt2'] = df_test['text']\n",
    "df_test['clean_txt2'] = df_test['clean_txt2'].map(\n",
    "    lambda x: word_embedding(x) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "141577fb",
   "metadata": {
    "id": "f6861156",
    "outputId": "86ebfb8a-c5fd-48ab-cbd6-c19bf8c8af7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>clean_txt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>group friend began volunt homeless shelter nei...</td>\n",
       "      <td>group of friends began to volunteer at homeles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>british prime minist theresa may nerv attack f...</td>\n",
       "      <td>British Prime Minister Theresa May on Nerve At...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>1961 goodyear releas kit allow ps2s brought he...</td>\n",
       "      <td>In , Goodyear released kit that allows PS to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>happi birthday bob barker price right host lik...</td>\n",
       "      <td>Happy Birthday , Bob Barker ! The Price Is Rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>obama nation innoc cop unarm young black men d...</td>\n",
       "      <td>Obama to Nation Innocent Cops and Unarmed Youn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  A group of friends began to volunteer at a hom...   \n",
       "1  British Prime Minister @Theresa_May on Nerve A...   \n",
       "2  In 1961, Goodyear released a kit that allows P...   \n",
       "3  Happy Birthday, Bob Barker! The Price Is Right...   \n",
       "4  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...   \n",
       "\n",
       "                                           clean_txt  \\\n",
       "0  group friend began volunt homeless shelter nei...   \n",
       "1  british prime minist theresa may nerv attack f...   \n",
       "2  1961 goodyear releas kit allow ps2s brought he...   \n",
       "3  happi birthday bob barker price right host lik...   \n",
       "4  obama nation innoc cop unarm young black men d...   \n",
       "\n",
       "                                          clean_txt2  \n",
       "0  group of friends began to volunteer at homeles...  \n",
       "1  British Prime Minister Theresa May on Nerve At...  \n",
       "2  In , Goodyear released kit that allows PS to b...  \n",
       "3  Happy Birthday , Bob Barker ! The Price Is Rig...  \n",
       "4  Obama to Nation Innocent Cops and Unarmed Youn...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'clean_txt', 'clean_txt2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "976e26d1",
   "metadata": {
    "id": "n-c__ygWzLU3"
   },
   "outputs": [],
   "source": [
    "#split val and test set\n",
    "X1 = df['clean_txt2']\n",
    "y1 = df['label']\n",
    "X, X_ts, y, y_ts = train_test_split(X1, y1 , test_size = 0.2 , stratify = y1, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y , test_size = 0.2 , stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c1132fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1647186340537,
     "user": {
      "displayName": "asmaa kandil",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh80UJPWPEk1itZhs5DtBbaN9KVmPp_FF7Pu2KE=s64",
      "userId": "14088357115434192972"
     },
     "user_tz": -120
    },
    "id": "d1e6f6b6",
    "outputId": "b8383de6-081e-4c81-ecb1-80808c70a42c"
   },
   "outputs": [],
   "source": [
    "#create the pipeline\n",
    "full_pipe = Pipeline(steps=[\n",
    "    ('vectorizor', TfidfVectorizer(stop_words='english' )) , \n",
    "    ('my_model', LogisticRegression( verbose = 0)) \n",
    "])\n",
    "\n",
    "#define parameter grid for the tuning\n",
    "param_grid = {\n",
    "     #hypers of the Logistic Reg\n",
    "     'my_model__penalty': [None, 'l2'],\n",
    "    'my_model__C':[1.0, 0.8],\n",
    "    'my_model__max_iter': [10000], # so the model can converge\n",
    "   \n",
    "     #best combination from Trial 3\n",
    "    'vectorizor__analyzer': ['char_wb'],\n",
    "    'vectorizor__ngram_range': [(3,4)], \n",
    "    'vectorizor__max_df': [0.3],  \n",
    "    \"vectorizor__min_df\": [23] }\n",
    "\n",
    "# define the predifened validation set\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "pre_defined_split = PredefinedSplit(test_fold = split_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c79a58c",
   "metadata": {
    "id": "L2zag53e9QmT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2 fits failed out of a total of 4.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\asmaa\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.87215619        nan 0.87112095]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of the cv: 0.8721561933852436\n",
      "best Hyper set: {'my_model__C': 1.0, 'my_model__max_iter': 10000, 'my_model__penalty': 'l2', 'vectorizor__analyzer': 'char_wb', 'vectorizor__max_df': 0.3, 'vectorizor__min_df': 23, 'vectorizor__ngram_range': (3, 4)}\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(\n",
    "    full_pipe, param_grid, cv = pre_defined_split , scoring=\"roc_auc\" , verbose=0)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print('best score of the cv:', grid_search.best_score_)\n",
    "print('best Hyper set:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f23ce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      6435\n",
      "           1       0.77      0.77      0.77      5519\n",
      "\n",
      "    accuracy                           0.79     11954\n",
      "   macro avg       0.79      0.79      0.79     11954\n",
      "weighted avg       0.79      0.79      0.79     11954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat = grid_search.predict(X_ts)\n",
    "report = classification_report(y_ts, y_hat)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98a2457d",
   "metadata": {
    "id": "sViXZmVX18bO"
   },
   "outputs": [],
   "source": [
    "#predict for the test file data\n",
    "# and save to file\n",
    "y_out = random_search.predict_proba(df_test['clean_txt'])\n",
    "\n",
    "dummy = pd.DataFrame({'id': df_test['id'],'label': y_out[:,1]})\n",
    "dummy.to_csv(\"T5.3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be13dd",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "<a class=\"anchor\" id=\"conc\"></a>\n",
    "\n",
    "No thing makes sense, What you think you know is probably not true, studing is never enough.\n",
    "\n",
    "That being said, The best model under these 6 Trials was \n",
    "- Logistic regression with C=1.0, penalty=l2, char_wb level vectorizer with ngram(3,4), All with no preprocessing, just raw data. It took 2 min under the grid search to finish, which was faster than others which took up to 4 and 6 minutes.\n",
    "\n",
    "The embedding i did was week. In coming Trials i hope to experience something like word2vec, I've just learned about it today, it will be very interesting to create a vector for the similarities of words. The word vectors doesn't quite mean the machine can understand the meaning of words, but the similarities between them,"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C3_npl_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
